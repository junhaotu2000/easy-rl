{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# suppress all DeprecationWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import gym\n",
    "from gym import envs\n",
    "import time\n",
    "from tqdm import tnrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>>\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "# gym.make用以初始化环境，整个gym的入口函数\n",
    "env = gym.make(env_name)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4']\n"
     ]
    }
   ],
   "source": [
    "# 查看gym已有的所有环境\n",
    "env_specs = envs.registry.values()\n",
    "envs_ids = [env_spec.id for env_spec in env_specs]\n",
    "print(envs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 env成员\n",
    "- 环境定义了动作空间及状态空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 动作空间（这个环境下是离散的 - env.spaces.Discrete class）\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从动作空间中采样一个随机动作\n",
    "env.action_space.sample()\n",
    "# 检查一个动作是否在动作空间中\n",
    "env.action_space.contains([0.5, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观测空间 （这个环境下是连续的 - env.spaces.Box class）\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3331118e+00, -6.9434040e+37, -3.4935260e-01,  7.5309936e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观测空间的下边界\n",
    "env.observation_space.low\n",
    "# 观测空间的上边界\n",
    "env.observation_space.high\n",
    "# 从观测空间中随机采样一个观测值\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 用action与env交互\n",
    "- 单步更新需要(step(action)）:\n",
    "$$\n",
    "R(s_t, a_t) = r_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(s_t, a_t) = s_{t+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "score = 0\n",
    "env.reset()\n",
    "\n",
    "# 一次实验\n",
    "while not done:\n",
    "    # 环境开始渲染\n",
    "    env.render()\n",
    "    # policy - 随机选取一个动作\n",
    "    action = env.action_space.sample()\n",
    "    # 根据agent的action, 环境更新返回 observation, reward, done, info\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    # 更新reward\n",
    "    score += reward\n",
    "print(f'total reward: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每次epoch前要重置环境，重置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, total reward: 23.0\n",
      "epoch: 2, total reward: 23.0\n",
      "epoch: 3, total reward: 15.0\n",
      "epoch: 4, total reward: 26.0\n",
      "epoch: 5, total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "# 设置epoch的次数\n",
    "for epoch in range(1, 5+1):\n",
    "    done = False\n",
    "    score = 0\n",
    "    # 每次实验前重置环境\n",
    "    env.reset()\n",
    "    \n",
    "    # 进行一次实验（epoch）\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f'epoch: {epoch}, total reward: {score}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 17.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "score = 0\n",
    "frame = []\n",
    "env.reset()\n",
    "\n",
    "while not done:\n",
    "    frame.append(env.render(mode='rgb_array'))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    time.sleep(0.5)\n",
    "print(f'total reward: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MountainCar-v0 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def decide(self, observation):\n",
    "        position, velocity = observation\n",
    "        lb = min(-0.09 * (position + 0.25) **2 + 0.03, \n",
    "                0.3 * (position + 0.9) **4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) **2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "        return action\n",
    "    \n",
    "    def learn(self, *args):\n",
    "        pass\n",
    "    \n",
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play(env, agent, render=False, train=False):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation, action, reward, done)\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        observation = next_observation\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eposide reward: -105.0\n"
     ]
    }
   ],
   "source": [
    "env.seed(3) # 设置随机种子，让结果可复现\n",
    "episode_reward = play(env, agent, render=True)\n",
    "print(f'eposide reward: {episode_reward}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c3c66c9b164935b323c130197c364e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: -106.1631\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "# 根据习惯，学术界一般把设置为eposide数量设置为100\n",
    "all_rewards = [play(env, agent, render=False) for _ in tnrange(10000)]\n",
    "env.close()\n",
    "print(f'mean reward: {np.mean(all_rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
